# Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions

Official repository for Interleaveâ€‘VLA, the first visionâ€‘languageâ€‘action (VLA) framework that understands interleaved imageâ€“text instructions and directly produces continuous actions in realâ€‘world scenarios.

## Overview ğŸ§­
![Overview](/assets/overview.png)

**Quick links**: ğŸ“„ [Paper](https://arxiv.org/abs/2505.02152) Â· ğŸŒ [Project Website](https://interleave-vla.github.io/Interleave-VLA-Anonymous/) Â· ğŸ“¦ [Dataset](https://huggingface.co/collections/Interleave-VLA/interleave-vla-dataset-6866a10654b16d02032db7a1) Â· ğŸ’» [Code](https://github.com/orgs/Interleave-VLA/repositories)

Interleaveâ€‘VLA is a flexible, modelâ€‘agnostic upgrade that extends stateâ€‘ofâ€‘theâ€‘art VLA models with minimal changes and strong zeroâ€‘shot generalization, achieving up to 2Ã— better outâ€‘ofâ€‘domain generalization to unseen objects compared with textâ€‘only VLA baselines.

## Get Started ğŸš€

Interleaveâ€‘VLA is built upon stateâ€‘ofâ€‘theâ€‘art VLA models. We provide two implementations:
### Interleaveâ€‘Ï€0

Train and evaluate:

 âœ… Documentation: [Interleaveâ€‘Ï€0](/open-pi-zero/doc/interleave_pi0.md) â€” complete and ready to use.

 ğŸ¤— Checkpoint on HuggingFace: [Interleaveâ€‘Ï€0 Checkpoint](https://huggingface.co/Interleave-VLA/interleave-pi0-bridge).

### Interleaveâ€‘OpenVLA
Guide coming soon:

 ğŸ› ï¸ Documentation: [Interleaveâ€‘OpenVLA](/openvla/doc/interleave_openvla.md) â€” in progress â³.

 ğŸ“¥ Checkpoint â€” coming soon â³.

## Roadmap ğŸ—ºï¸
- [x] Release Interleaveâ€‘Ï€0 code
- [x] Release Interleaveâ€‘Ï€0 documentation
- [x] Release Interleaveâ€‘Ï€0 checkpoint
- [x] Release Interleaveâ€‘OpenVLA code
- [ ] Release Interleaveâ€‘OpenVLA documentation
- [ ] Release Interleaveâ€‘OpenVLA checkpoint

## Acknowledgements ğŸ™
This project builds upon the following works â¤ï¸:

- [open-pi-zero](https://github.com/allenzren/open-pi-zero.git)
- [OpenVLA](https://github.com/openvla/openvla)
- [InternVL](https://github.com/OpenGVLab/InternVL)

## Support ğŸ’¬
ğŸ“§ Questions or feedback? Contact Cunxin Fan at <alfayoung2004@gmail.com> or open an issue.

## Citation ğŸ“š
If you find our work helpful, please consider citing our paper:
```
@misc{fan2025interleavevlaenhancingrobotmanipulation,
      title={Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions},
      author={Cunxin Fan and Xiaosong Jia and Yihang Sun and Yixiao Wang and Jianglan Wei and Ziyang Gong and Xiangyu Zhao and Masayoshi Tomizuka and Xue Yang and Junchi Yan and Mingyu Ding},
      year={2025},
      eprint={2505.02152},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2505.02152},
}
```

If you find this repository helpful, please give it a â­ â€” thanks! ğŸ™Œ